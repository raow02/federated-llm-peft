# Federated LLM Fine-tuning with LoRA

## Project Overview

This project focuses on **Federated Fine-tuning of Large Language Models (LLMs)** using **Low-Rank Adaptation (LoRA)**. It allows for efficient adaptation of pre-trained LLMs like Llama-3.2-1B on distributed datasets residing on multiple clients, simulating a federated learning environment.

The primary goal is to explore different federated learning strategies for LLM fine-tuning, including:

*   **Federated Averaging (FedAvg):** Federated learning with FedAvg (`federation_mode="homo"`).
*   **Sequential Training:** Clients train one after another, passing the model state sequentially (`federation_mode="seq"`).
*   **Independent Training:** Each client trains independently for comparison (`federation_mode="none"`).

This project is based on the work from [FederatedGPT-Shepherd](https://github.com/JayZhang42/FederatedGPT-Shepherd).


## Requirements

```bash
pip install -r requirements.txt
```


## Usage

### Step 0: Data Preparation (Optional)

If you want to allocate or re-split your own dataset for federated clients:

```bash
python client_data_allocation.py [options]
```

*   **Description:** This script partitions a dataset into local training sets for each client and a global test set.
*   **Tunable Parameters:**
    *   `--input_file`: Path to the raw dataset (default: `./data/new-databricks-dolly-15k.json`).
    *   `--output_root`: Directory to save partitioned data (default: `./data`).
    *   `--dataset_name`: Name of the dataset (default: `dataset1`)
    *   `--train_per_cat`: Number of samples per category for training (default: 300).
    *   `--test_per_cat`: Number of samples per category for the global test set (default: 200).
    *   `--seed`: Random seed for reproducibility (default: 42).

### Step 1: Training

Run the training script:

```bash
bash script/train.sh
```

*   **Description:** This script launches the main fine-tuning process (`train.py`) based on the configuration set within the script.
*   **Tunable Parameters (inside `script/train.sh`):**
    *   `exp_name`: A name for your experiment run (e.g., `fedavg-1B`, `seq-1B`, `none-1B`).
    *   `base_model`: The Hugging Face model identifier for the base LLM (e.g., `meta-llama/Llama-3.2-1B`).
    *   `data_path`: Path to the root data directory.
    *   `model_dir`: Directory where trained model adapters will be saved.
    *   `num_communication_rounds`: Total number of federated learning rounds.
    *   `client_selection_frac`: Fraction of clients to participate in each round.
    *   `federation_mode`: The training strategy:
        *   `homo`: Federated Averaging with LoRA.
        *   `seq`: Sequential training (clients train one after another).
        *   `none`: Individual training (each client trains independently from scratch).

### Step 2: Generation

Generate predictions using the trained models:

```bash
bash script/generate.sh
```

*   **Description:** This script runs inference (`generate.py`) using a trained model (either global or a specific client's) on the test set.
*   **Tunable Parameters (inside `script/generate.sh`):**
    *   `exp_name`: The name of the experiment (must match the training step).
    *   `base_model`: The base model used during training.
    *   `model_dir`: Directory where trained models were saved.
    *   `is_global_model`: Set to `true` to generate using the aggregated global model, `false` for a specific client model.
    *   `client_id`: The client ID to use if `is_global_model` is `false`.
    *   `communication_rounds`: Total number of federated learning rounds (must match the training step).
    *   `test_file_path`: Path to the test dataset (e.g., `./data/dataset1/dolly_test_200.jsonl`).
    *   `prediction_dir`: Directory where the generated output files (`.jsonl`) will be saved.
    *   `batch_size`: Batch size for inference.
    *   `is_base_model`: Set to `true` to generate using *only* the base model (no fine-tuning adapters applied) for baseline comparison.

### Step 3: Evaluation

Evaluate the generated predictions using LLM-based evaluation and ROUGE scores.

**A. LLM-based Evaluation:**

```bash
bash script/evaluate_llm.sh
```

*   **Description:** Compares the outputs of our proposed federated model against a baseline model's outputs using a LLM (like GPT-4.1) as a judge.
*   **Tunable Parameters (inside `script/evaluate_llm.sh`):**
    *   `api_key`: Your OpenAI API key. 
    *   `gpt_model`: The specific GPT model to use for judging (e.g., `gpt-4.1`).
    *   `is_global_model`: Set to `true` if evaluating the global model's predictions, `false` for a client's.
    *   `exp_name`: The experiment name.
    *   `communication_rounds`: The training round number corresponding to the predictions being evaluated.
    *   `proposed_file`: Path to the prediction file generated by our proposed federated model (e.g., `homo-1B/20/global_output.jsonl`).
    *   `prediction_dir`: The base directory containing *all* prediction outputs (e.g., `./predictions`).
    *   `evaluation_dir`: Directory to save the LLM evaluation results (e.g., `./evaluations_llm`).
    *   `batch_size`: Test sample batch size for making requests to the OpenAI API.

**B. ROUGE Evaluation:**

```bash
bash script/evaluate_rouge.sh
```

*   **Description:** Calculates ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-Lsum) by comparing the generated predictions against the ground truth reference answers in the test set.
*   **Tunable Parameters (inside `script/evaluate_rouge.sh`):**
    *   `is_global_model`: Set to `true` if evaluating the global model's predictions, `false` for a client's.
    *   `exp_name`: The experiment name.
    *   `communication_rounds`: The training round number corresponding to the predictions being evaluated.
    *   `target_file`: Path to the test dataset containing the ground truth answers (e.g., `./data/dataset1/dolly_test_200.jsonl`).
    *   `prediction_dir`: The base directory containing prediction outputs (e.g., `./predictions`).
    *   `evaluation_dir`: Directory to save the ROUGE evaluation results  (e.g., `./evaluations_rouge`).


## Output Structure

*   **Models:** Saved under `<model_dir>/<exp_name>/<round_number>/`
    *   Global models: `global_adapter_model.bin`, `adapter_config.json`
    *   Client models: `client_<i>/adapter_model.bin`, `client_<i>/adapter_config.json`
*   **Predictions:** Saved under `<prediction_dir>/<exp_name>/<round_number>/`
    *   `global_output.jsonl` or `client_<i>_output.jsonl`
*   **Evaluations:**
    *   LLM Evals: `<evaluation_llm_dir>/<exp_name>/<round_number>/` (`global_results.json` or `client_<i>_results.json`)
    *   ROUGE Evals: `<evaluation_rouge_dir>/<exp_name>/<round_number>/` (`global_output_rouge.json` or `client_<i>_output_rouge.json`)

